#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
query_kb.py - KB ê²€ìƒ‰ ìœ í‹¸ë¦¬í‹° (run_sop.py ì—°ë™ìš©)

ì—­í• :
  1. build_kb.pyê°€ ìƒì„±í•œ KBë¥¼ ê²€ìƒ‰
  2. run_sop.pyê°€ Agentì—ê²Œ ê·¼ê±°ë¥¼ ì œê³µí•  ë•Œ ì‚¬ìš©
  3. ì¿¼ë¦¬ â†’ ê´€ë ¨ ì²­í¬ ë°˜í™˜ (RAG)

ì‚¬ìš©ë²•:
  # CLI í…ŒìŠ¤íŠ¸
  python query_kb.py --config config.yaml --query "3D reconstruction ê²½í—˜"
  
  # Python (run_sop.pyì—ì„œ)
  from query_kb import KBSearcher
  searcher = KBSearcher(config)
  results = searcher.search("diffusion model", top_k=5)
"""

import os
import json
import yaml
import argparse
from typing import List, Dict, Any, Optional

import numpy as np
import faiss
from openai import OpenAI


class KBSearcher:
    """Knowledge Base ê²€ìƒ‰ê¸°"""
    
    def __init__(self, config: Dict):
        """
        Args:
            config: config.yaml ì „ì²´ ë˜ëŠ” í•„ìš”í•œ ë¶€ë¶„
        """
        self.config = config
        
        # API í´ë¼ì´ì–¸íŠ¸
        api_key = config.get("openai", {}).get("api_key", "").strip()
        if not api_key:
            api_key = os.environ.get("OPENAI_API_KEY", "")
        self.client = OpenAI(api_key=api_key)
        
        # ê²½ë¡œ
        kb_dir = config.get("paths", {}).get("kb_dir", "kb")
        
        # RAG ì„¤ì •
        rag_cfg = config.get("rag", {})
        self.embed_model = rag_cfg.get("embed_model", "text-embedding-3-large")
        self.default_top_k = rag_cfg.get("top_k", 6)
        self.max_evidence_chars = rag_cfg.get("max_evidence_chars", 1800)
        self.min_importance = rag_cfg.get("min_importance", 0.0)
        
        # KB ë¡œë“œ
        self.kb_dir = kb_dir
        self.chunks = self._load_chunks()
        self.index = self._load_index()
        self.summary = self._load_summary()
        self.papers = self._load_papers()
    
    def _load_chunks(self) -> List[Dict]:
        path = os.path.join(self.kb_dir, "kb_chunks.jsonl")
        chunks = []
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                chunks.append(json.loads(line))
        return chunks
    
    def _load_index(self):
        path = os.path.join(self.kb_dir, "kb_index.faiss")
        return faiss.read_index(path)
    
    def _load_summary(self) -> Dict:
        path = os.path.join(self.kb_dir, "kb_summary.json")
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def _load_papers(self) -> List[Dict]:
        path = os.path.join(self.kb_dir, "kb_papers.json")
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í•µì‹¬ ê²€ìƒ‰ ë©”ì„œë“œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def search(self, query: str, top_k: Optional[int] = None,
               min_importance: Optional[float] = None,
               section_filter: Optional[List[str]] = None) -> List[Dict]:
        """
        ì¿¼ë¦¬ë¡œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            min_importance: ìµœì†Œ ì¤‘ìš”ë„ í•„í„°
            section_filter: ì„¹ì…˜ í•„í„° (ì˜ˆ: ["contributions", "meta"])
        
        Returns:
            [{"score": float, "chunk": Dict}, ...]
        """
        top_k = top_k or self.default_top_k
        min_importance = min_importance if min_importance is not None else self.min_importance
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        resp = self.client.embeddings.create(model=self.embed_model, input=[query])
        query_vec = np.array([resp.data[0].embedding], dtype=np.float32)
        faiss.normalize_L2(query_vec)
        
        # ê²€ìƒ‰
        k = min(top_k * 3, len(self.chunks))
        scores, indices = self.index.search(query_vec, k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < 0:
                continue
            
            chunk = self.chunks[idx]
            
            if chunk.get("importance", 0) < min_importance:
                continue
            if section_filter and chunk.get("section") not in section_filter:
                continue
            
            results.append({"score": float(score), "chunk": chunk})
            
            if len(results) >= top_k:
                break
        
        return results
    
    def get_evidence_for_claim(self, claim: str, top_k: int = 3) -> List[Dict]:
        """
        ì£¼ì¥ì— ëŒ€í•œ ê·¼ê±° ê²€ìƒ‰ (ìê¸°ì†Œê°œì„œ ì‘ì„±ìš©)
        
        Returns:
            [{"text": str, "source": str, "relevance": float, "importance": float}, ...]
        """
        results = self.search(claim, top_k=top_k, min_importance=0.2)
        
        evidences = []
        for r in results:
            chunk = r["chunk"]
            evidences.append({
                "text": chunk["text"],
                "source": chunk.get("meta", {}).get("title", "Unknown"),
                "section": chunk.get("section"),
                "relevance": r["score"],
                "importance": chunk.get("importance", 0)
            })
        
        return evidences
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìš”ì•½ ì •ë³´ ì ‘ê·¼
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def get_researcher_profile(self) -> str:
        return self.summary.get("researcher_profile", "")
    
    def get_top_keywords(self, n: int = 10) -> List[str]:
        return [kw for kw, _ in self.summary.get("top_keywords", [])[:n]]
    
    def get_research_areas(self) -> List[str]:
        return self.summary.get("research_areas", [])
    
    def get_representative_papers(self, n: int = 3) -> List[Dict]:
        return self.summary.get("representative_papers", [])[:n]
    
    def get_technical_expertise(self) -> List[str]:
        return self.summary.get("technical_expertise", [])
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # run_sop.py ì—°ë™ìš©
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def get_context_for_requirements(self, requirements: List[str]) -> Dict[str, Any]:
        """
        íšŒì‚¬ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            requirements: ["diffusion model", "self-motivated", ...]
        
        Returns:
            Agent í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•  ì»¨í…ìŠ¤íŠ¸
        """
        context = {
            "researcher_profile": self.get_researcher_profile(),
            "research_areas": self.get_research_areas(),
            "top_keywords": self.get_top_keywords(15),
            "representative_papers": self.get_representative_papers(5),
            "technical_expertise": self.get_technical_expertise(),
            "requirement_matches": {}
        }
        
        for req in requirements:
            evidences = self.get_evidence_for_claim(req, top_k=3)
            context["requirement_matches"][req] = evidences
        
        return context
    
    def format_context_for_prompt(self, context: Dict[str, Any]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ ì‚½ì…ìš© ë¬¸ìì—´ë¡œ ë³€í™˜"""
        lines = []
        
        lines.append("## ì—°êµ¬ì í”„ë¡œí•„")
        lines.append(context["researcher_profile"])
        
        lines.append("\n## ì—°êµ¬ ë¶„ì•¼")
        lines.append(", ".join(context["research_areas"]))
        
        lines.append("\n## í•µì‹¬ í‚¤ì›Œë“œ")
        lines.append(", ".join(context["top_keywords"]))
        
        lines.append("\n## ê¸°ìˆ  ì „ë¬¸ì„±")
        lines.append(", ".join(context["technical_expertise"]))
        
        lines.append("\n## ëŒ€í‘œ ë…¼ë¬¸")
        for p in context["representative_papers"]:
            lines.append(f"- {p['title']} ({p['venue']}, {p['year']})")
            if p.get("contribution"):
                lines.append(f"  â†’ {p['contribution']}")
        
        lines.append("\n## ìš”êµ¬ì‚¬í•­ë³„ ê·¼ê±°")
        for req, evidences in context["requirement_matches"].items():
            lines.append(f"\n### {req}")
            for ev in evidences:
                text = ev["text"][:300] + "..." if len(ev["text"]) > 300 else ev["text"]
                lines.append(f"- [{ev['source']}] {text}")
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        result = "\n".join(lines)
        if len(result) > self.max_evidence_chars:
            result = result[:self.max_evidence_chars] + "\n...(truncated)"
        
        return result
    
    def get_evidence_text(self, query: str, top_k: int = 5) -> str:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ ê·¼ê±°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (Agent í”„ë¡¬í”„íŠ¸ ì‚½ì…ìš©)
        """
        results = self.search(query, top_k=top_k)
        
        if not results:
            return "[ê´€ë ¨ ê·¼ê±° ì—†ìŒ]"
        
        parts = []
        total_chars = 0
        
        for i, r in enumerate(results, 1):
            chunk = r["chunk"]
            text = chunk["text"]
            
            # ê¸¸ì´ ì œí•œ
            if total_chars + len(text) > self.max_evidence_chars:
                remaining = self.max_evidence_chars - total_chars
                if remaining > 200:
                    text = text[:remaining] + "..."
                else:
                    break
            
            parts.append(f"[ê·¼ê±° {i}] (ê´€ë ¨ë„: {r['score']:.2f})\n"
                        f"ì¶œì²˜: {chunk.get('meta', {}).get('title', 'Unknown')}\n"
                        f"{text}")
            total_chars += len(text)
        
        return "\n\n---\n\n".join(parts)


# ============================================================
# CLI
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="KB ê²€ìƒ‰")
    parser.add_argument("--config", type=str, default="config.yaml")
    parser.add_argument("--query", type=str, required=True)
    parser.add_argument("--top_k", type=int, default=5)
    parser.add_argument("--mode", type=str, default="search",
                        choices=["search", "evidence", "profile", "context"])
    args = parser.parse_args()
    
    with open(args.config, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    searcher = KBSearcher(config)
    
    if args.mode == "search":
        results = searcher.search(args.query, top_k=args.top_k)
        print(f"\nğŸ” ê²€ìƒ‰: '{args.query}' ({len(results)}ê°œ ê²°ê³¼)")
        print("=" * 50)
        for i, r in enumerate(results, 1):
            chunk = r["chunk"]
            print(f"\n[{i}] Score: {r['score']:.3f} | Importance: {chunk.get('importance', 0):.2f}")
            print(f"Source: {chunk.get('meta', {}).get('title', 'Unknown')}")
            print(f"Text: {chunk['text'][:200]}...")
    
    elif args.mode == "evidence":
        evidences = searcher.get_evidence_for_claim(args.query, top_k=args.top_k)
        print(f"\nğŸ“‹ ê·¼ê±°: '{args.query}'")
        print("=" * 50)
        for i, ev in enumerate(evidences, 1):
            print(f"\n[{i}] {ev['source']}")
            print(f"  ê´€ë ¨ë„: {ev['relevance']:.3f}, ì¤‘ìš”ë„: {ev['importance']:.2f}")
            print(f"  {ev['text'][:300]}...")
    
    elif args.mode == "profile":
        print("\nğŸ‘¤ ì—°êµ¬ì í”„ë¡œí•„")
        print("=" * 50)
        print(searcher.get_researcher_profile())
        print(f"\nì—°êµ¬ ë¶„ì•¼: {', '.join(searcher.get_research_areas())}")
        print(f"í‚¤ì›Œë“œ: {', '.join(searcher.get_top_keywords(10))}")
    
    elif args.mode == "context":
        # ì¿¼ë¦¬ë¥¼ ìš”êµ¬ì‚¬í•­ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
        requirements = [r.strip() for r in args.query.split(",")]
        context = searcher.get_context_for_requirements(requirements)
        print("\nğŸ“ ì»¨í…ìŠ¤íŠ¸")
        print("=" * 50)
        print(searcher.format_context_for_prompt(context))


if __name__ == "__main__":
    main()
